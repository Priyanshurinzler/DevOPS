23 May

Currently working as part of AGILE methodology, tracking everything on JIA, except ahoc production requests, use XMatters for it
manage infrastructure and configurations using terraform and ansible
delivers apps as containers on kubernetes
monitor the pods and infrastructure



1)	ApigeeX
	Apigee X is Google Cloud's API management platform that allows organizations to develop, manage, secure, and scale APIs (Application Programming Interfaces). 
	It’s the next generation of Apigee, tightly integrated with Google Cloud services
	
	
2)	Provisioned GCP and Azure hosts
		we used the Azure Portal / Google Cloud Console
		Gcloud / az CLI 
		
		Together using Terraform Modules
		
		code :
			# For GCP
				provider "google" {
				project = "my-project-id"
				region  = "us-central1"
				}
	
				resource "google_compute_instance" "default" {
				name         = "gcp-instance"
				machine_type = "e2-medium"
				zone         = "us-central1-a"
				...
				}
				
				# For Azure
				provider "azurerm" {
				features = {}
				}
				
				resource "azurerm_linux_virtual_machine" "example" {
				name                = "azure-vm"
				resource_group_name = azurerm_resource_group.example.name
				location            = azurerm_resource_group.example.location
				...
				}
	
3)	Managing Kubernetes Cluster Setup
	Managing Kubernetes clusters, whether on GCP (GKE) or Azure (AKS), involves PROVISIONING THE CLUSTER, configuring access, deploying workloads, and setting up monitoring and scaling.
	
	Step 1 : Provisioning the Cluster
			create a resource group
			create a cluster
			connect the resource group to the cluster
	
	Step 2 : Cluster Configuration Management Tasks
			Kubeconfig: Use ~/.kube/config to authenticate and access clusters.
			RBAC
			Network Policies : restrict communication between pods
			Resource Quotas and limits
	
	Step 3 : Deployment and Scaling
			Kubectl or gitOPS tool like ARGOCD
			kubectl apply -f deployment.yaml
			
			enable horizontal pod autocaling
			kubectl autoscale deployment myapp --cpu-percent=80 --min=2 --max=10
	
	Step 4 : Monitoring and Logging
			Prometheus + Grafana
			Cloud native monitoring tool >>> Cloudwatch, Azure Monitor, Google Cloud Operations
			
	Step 5 : Security
			Pod Security
			TLS & Secrets
			
	Step 6 : Troubleshooting
			Pod Issues : kubectl describe pod <podname>
						 kubectl logs <podname>
			Events : 	 kubectl get events --sort-by=.metadata.creationTimestamp
			
			CrashLoopBackOff : Common error
			
			CrashLoopBackOff is a common Kubernetes pod error that indicates a container starts, crashes, and is being repeatedly restarted by the kubelet. It typically means the container is exiting too quickly, and Kubernetes is backing off before retrying.
					
					Crash: The container terminated with a non-zero exit code.
		
					Loop: Kubernetes keeps restarting it.
		
					BackOff: Kubernetes is waiting longer and longer between restart attempts.
					
			
			Diagnose CrashLoopBackOff :
				Check pod Status
				Check logs
				Descrie the pod >>> This shows reason for crash, exit codes, restart count, Events
				Check Liveness/Readiness Probes
				Look for Exit codes

4)	Led OS Migrations from CentOS to RockyLinux for DC & GCP hosts
	
	Migrating from CentOS to Rocky Linux in a Data Center (DC) / GCP environment is a critical infrastructure upgrade driven by CentOS 8's end-of-life (EOL) and the need for a stable, enterprise-grade RHEL-compatible OS. Rocky Linux, being a community-driven rebuild of RHEL, is a common choice for enterprises seeking long-term stability
	
	STEP by STEP Migration Guide
	
	Step 1 : Migration Strategy overview
		Approach					Description
		In-place Upgrade			Convert existing CentOS 8 servers to Rocky Linux without reinstallation.
		Rebuild + Reinstall			Fresh install Rocky Linux and reconfigure manually or via automation (Ansible, Terraform, etc.)
		Phased Rollout				Migrate non-critical workloads first, then roll out to production nodes in waves.
		Backup/Restore				Backup config/data, clean install Rocky, restore services.
		
	Step 2 : Pre-Migration Checklist
		Inventory all CentOS servers
		Verify compatiblity with RockyLinux
		Document installed packages, running services, network configs, cron jobs, scripts
		Snapshot / backups of containers or VMs
		Notify the stakeholders
		
	Step 3 : In-Place migration using migrate2rocky.sh
		RockyLinux offers a scripted migratio for CentOS 8
		It:
			Replaces CentOS repos with Rocky ones
			Updates RPMs
			Replaces branding
			Cleans up caches
	
	Step 4 : Post Migration Validation
			Check OS version and repo files
			Verify critical services (Nginx, Apache, Docker, etc.)
			Run monitoring agents
			Rejoin config management
			Test custom scripts, cron jobs
			Run compliance scans
			
	NOTE : OS Migrations on GCP are more Cloud specific tasks like handling custom images, updating instance templates, and ensuring GCP services (like OS Login, Stackdriver agents, etc.) remain functional post-migration.
	
5)	Developed Terraform modules for resource provisioning in GCP
	
	Developing Terraform modules for GCP resource provisioning is all about creating reusable, scalable, and standardized building blocks that can be version-controlled and consumed across projects or teams.
	
	Step 1 : Understanding what to modularize
		Commonly reused resources / logical groupings
			VPC / subnets
			GKE clusters & node pools
			Service accounts and IAM bindings
			Compute instances with firewall
	
	Step 2 : Terraform Module Structure
		gcp-modules/
		└── gke/
			├── main.tf         # Contains resource definitions
			├── variables.tf    # Input variables for the module
			├── outputs.tf      # Outputs to expose to caller
			└── README.md       # Documentation

	Step 3 : Define the Module
		variables.tf ( project id, network name, region ) 
		main.tf	( resource definition )
		outputs.tf ( network id )
		
	Step 4 : Use the module
		In a root terraform project 
	
	Step 5 : Publish internally for everyone to use
	
	
	Tooling & Linting
		terraform fmt 		– format code
		terraform validate 	– validate syntax
		tflint 				– lint for GCP best practices
		pre-commit 			– enforce checks
		terraform-docs 		– auto-generate module documentation
		
6) 	Decommissioning Unused resources, deleting Kubernetes namespaces, Right Sizing GCP compute instances
	
	Critical COST OPTIMIZATION and CLOUD HYGIENE practices in DevOps and cloud operations.
	
	A : Decommissioning Unused resources
		Step 1 : Identify unused resources
			Used GCP Recommender >> Suggests idle VM instances, disks, IPs 
			Billing Reports
			Cloud Asset inventory
			
			Decommission those resources using the following
				# Delete a disk
				gcloud compute disks delete DISK_NAME --zone=us-central1-a
				
				# Release unused IP
				gcloud compute addresses delete IP_NAME --region=us-central1
				
			Can also use Terraform -target to delete specific unused resources
				terraform destroy -target=google_compute_instance.old_vm
	
	B : Deleting Kubernetes namespaces
		Identify stale namespaces
			kubectl get namespaces --show-labels
			here, we can check for 
						age,
						no active pods or deployments
		Then Delete the namespaces
			Kubectl delete namespace <namespace-name>
			
			
		Best Practices
			Label namespaces with env, owner, and expiry tags.
			
			Set auto-cleanup jobs for non-prod clusters.
			
	C : Right Sizing GCP compute instances
		Identify oversized VMs
			Use recommender ( Automatic )
			
			Manual >>> Check for CPU / Memory Usage with monitoring
			
		Resize the VM
			Stop the VM
			Make change
			Start the VM
			
			if it is part of MIG { Managed Instance Group }
			Use instance template >>> rolling update MIG

7) 	Automating IAM Resource provisioning using Terraform
	
	Automating IAM resource provisioning using Terraform on GCP ensures secure, consistent, and auditable access control across the cloud infrastructure.
	
	Step 1 : Determining what can you provision
		Service Account, Service Account Key
	
	Step 2 : Create service accounts with Terraform
		
	Step 3 : Assign IAM roles to these service accounts
	
	Step 4 : Create custom IAM role
	
	Step 5 : Make it a resuable module
